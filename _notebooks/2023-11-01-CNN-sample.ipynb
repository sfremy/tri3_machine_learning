{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "toc: true\n",
    "comments: true\n",
    "layout: post\n",
    "title: Personal Project - Convolutional Neural Network\n",
    "description: CNN development \n",
    "type: hacks\n",
    "courses: { csp: {week: 10, categories: [4.A]} }\n",
    "categories: [C1.4]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for Planets with Convolutional Neural Networks\n",
    "<p>This is a section of code from a AI-based scientific investigation of circumbinary planets. I will not explain the exact specifics of the code objectives since they are irrelevant to this class but all the features developed are here. Note that this does NOT work in VSCode - an excessive number of external dependencies are necessary and I am not working on it in here.</p>\n",
    "\n",
    "#### Key Features:\n",
    "- Passes negative and positive training data in numpy .npz files to a custom Keras convolutional neural network\n",
    "- Manufactures training datasets which the CNN can use\n",
    "- Trains the neural network using Adam optimizer\n",
    "- Allows user to select device from CUDA visible options and determine training parameters\n",
    "- Trains and saves a neural network model using all parameters passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/dn_3qv0s6dndm54j9k1xw61h0000gn/T/ipykernel_7115/2357884589.py:11: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "from os.path import join as opj\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import astropy\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "'''\n",
    "Imports packages of functions from transit_utils and model_training_toolkit, which are my own libraries\n",
    "\n",
    "import transit_utils as utils\n",
    "import model_training_toolkit as modelkit\n",
    "'''\n",
    "\n",
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a hardware device on which to run the CNN training\n",
    "\n",
    "proc_hardware_choice = 0\n",
    "\n",
    "\n",
    "if proc_hardware_choice == 0:\n",
    "    proc_hardware_name = '/gpu:0'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    \n",
    "elif proc_hardware_choice == 1:\n",
    "    proc_hardware_name = '/gpu:1'\n",
    "    \n",
    "elif proc_hardware_choice == 2:\n",
    "    proc_hardware_name = '/cpu:0'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "else:\n",
    "    print('ERROR: You didnt make a proper choice. Defaulting to CPU processing.')\n",
    "    proc_hardware_name = '/cpu:0'\n",
    "    \n",
    "print('Using hardware name =', proc_hardware_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These names are npz files I made through an iterative data synthesis model based on the other file\n",
    "# filelist = ['100K_transformed.npz', 'recovery/training_data/5K_foldless_trainingdata.npz']\n",
    "filelist = ['recovery/training_data/5K_transformed.npz']\n",
    "\n",
    "plc = []\n",
    "nlc = []\n",
    "\n",
    "for name in filelist:\n",
    "    p = np.load(name)['poslc']\n",
    "    n = np.load(name)['neglc']\n",
    "    \n",
    "    plc.append(p)\n",
    "    nlc.append(n)\n",
    "    \n",
    "positive_matrix = np.vstack(plc)\n",
    "negative_matrix = np.vstack(nlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a sample from the loaded data as a sanity check.\n",
    "\n",
    "i = np.random.randint(0, negative_matrix.shape[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(positive_matrix[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make testing and training datasets\n",
    "'''mk_set is a function from my package model_traning_toolkit which takes two sets of arrays \n",
    "(positive_matrix and negative_matrix) and randomly shuffles them into a 2D x_train and y_train array.\n",
    "'''\n",
    "x_train, y_train = modelkit.mk_set(positive_matrix, negative_matrix)\n",
    "\n",
    "\n",
    "# Reshape arrays to make tensorflow happy.\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training process parameters to pass to keras\n",
    "model_name = 'model_folded_v2' # Name the model to write to\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 80\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "log_dir = os.path.join(os.getcwd(), 'tb_logs')\n",
    "checkpoint_dir = os.path.join(os.getcwd(), 'data', 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras model layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Input, Flatten, Reshape,\n",
    "    GlobalMaxPooling1D, Lambda,\n",
    "    Conv2D, GaussianNoise,\n",
    "    Cropping2D, Concatenate, ZeroPadding2D)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Circular Convolutional Layer\n",
    "\n",
    "def CConv2D(filters, kernel_size, strides=(1, 1), activation='linear', padding='valid', kernel_initializer='glorot_uniform', kernel_regularizer=None):\n",
    "    \n",
    "    def CConv2D_inner(x):\n",
    "        # padding (see https://www.tensorflow.org/api_guides/python/nn#Convolution)\n",
    "        in_height = int(x.get_shape()[1])\n",
    "        in_width = int(x.get_shape()[2])\n",
    "\n",
    "        if (in_height % strides[0] == 0):\n",
    "            pad_along_height = max(kernel_size[0] - strides[0], 0)\n",
    "        else:\n",
    "            pad_along_height = max(\n",
    "                kernel_size[0] - (in_height % strides[0]), 0)\n",
    "        if (in_width % strides[1] == 0):\n",
    "            pad_along_width = max(kernel_size[1] - strides[1], 0)\n",
    "        else:\n",
    "            pad_along_width = max(kernel_size[1] - (in_width % strides[1]), 0)\n",
    "\n",
    "        pad_top = pad_along_height // 2\n",
    "        pad_bottom = pad_along_height - pad_top\n",
    "        pad_left = pad_along_width // 2\n",
    "        pad_right = pad_along_width - pad_left\n",
    "\n",
    "        # left and right side for padding\n",
    "        pad_left = Cropping2D(cropping=((0, 0), (in_width-pad_left, 0)))(x)\n",
    "        pad_right = Cropping2D(cropping=((0, 0), (0, in_width-pad_right)))(x)\n",
    "\n",
    "        # add padding to incoming image\n",
    "        conc = Concatenate(axis=2)([pad_left, x, pad_right])\n",
    "\n",
    "        # top/bottom padding options\n",
    "        if padding == 'same':\n",
    "            conc = ZeroPadding2D(padding={'top_pad': pad_top,\n",
    "                                          'bottom_pad': pad_bottom})(conc)\n",
    "        elif padding == 'valid':\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Padding \"{}\" does not exist!'.format(padding))\n",
    "\n",
    "        # perform the circular convolution\n",
    "        cconv2d = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                         strides=strides, activation=activation,\n",
    "                         padding='valid',\n",
    "                         kernel_initializer=kernel_initializer,\n",
    "                         kernel_regularizer=kernel_regularizer)(conc)\n",
    "\n",
    "        # return circular convolution layer\n",
    "        return cconv2d\n",
    "    return CConv2D_inner\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# The model\n",
    "\n",
    "def model_fun(data_shape):\n",
    "\n",
    "    inputs = Input(shape=data_shape)\n",
    "    \n",
    "    # New layer: Adds a small amount of noise to each sample during training\n",
    "    layer = GaussianNoise(stddev=1/100)(inputs)\n",
    "    \n",
    "    # New layer: Make tensor 2D for new 2D convolution layers\n",
    "    layer = Reshape((1, data_shape[0], 1))(layer)\n",
    "    \n",
    "    # New layer: Circular convolution layer\n",
    "    layer = CConv2D(\n",
    "        filters=128, \n",
    "        kernel_size=(1,3), \n",
    "        activation='linear',\n",
    "    )(layer)\n",
    "    \n",
    "    # New layer: Circular convolution layer\n",
    "    cconv2_filter_cnt = 256\n",
    "    layer = CConv2D(\n",
    "        filters=cconv2_filter_cnt, \n",
    "        kernel_size=(1,6), \n",
    "        activation='linear',\n",
    "    )(layer)\n",
    "    \n",
    "    # New layer: Make tensor 1D\n",
    "    layer = Reshape((data_shape[0], \n",
    "                     cconv2_filter_cnt))(layer)\n",
    "    \n",
    "    # New layer: Global pooling\n",
    "    layer = GlobalMaxPooling1D()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "\n",
    "    layer = Dense(256, activation='relu')(layer)\n",
    "    \n",
    "    layer = Dense(256, activation='relu')(layer)\n",
    "    \n",
    "    layer = Dense(1, activation='sigmoid')(layer)\n",
    "    \n",
    "    return Model(inputs, layer)\n",
    "\n",
    "# Create model\n",
    "model = model_fun(x_train.shape[1:])\n",
    "\n",
    "# Show model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test the model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD, Adadelta\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from livelossplot.tf_keras import PlotLossesCallback\n",
    "\n",
    "# Set optimizer and optimization options\n",
    "# optimizer = SGD(learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "optimizer = Adam(lr=5e-7)\n",
    "\n",
    "# Compile the model (required)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              #optimizer= 'adam',\n",
    "              optimizer = optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Set early stopping (during training) options\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=7, \n",
    "                               verbose=1, \n",
    "                               mode='auto')\n",
    "\n",
    "\n",
    "# Setup training checkpoint options\n",
    "model_checkpoint = ModelCheckpoint(opj(checkpoint_dir, str(model_name) + '.hdf5'), \n",
    "                                   monitor='val_loss', \n",
    "                                   save_best_only=True)\n",
    "\n",
    "\n",
    "# Define callbacks, which TF will call on after each training epoch finishes\n",
    "callbacks = [model_checkpoint,  # Make checkpoints\n",
    "             PlotLossesCallback(), # Show training progress (model accuracy and loss)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the keras model above through the training process with the specified dataset and training epochs\n",
    "\n",
    "print(proc_hardware_name)\n",
    "\n",
    "with tf.device(proc_hardware_name): # With the hardware chosen, train the model!\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs,\n",
    "              validation_split=0.3,\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks,\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights at the checkpoint which had the best results\n",
    "model.load_weights(opj(data_dir, 'models', str(model_name) + '.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "model.save(opj(data_dir, 'models', str(model_name) + '_save1.hd5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
